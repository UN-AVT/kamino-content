---
title: "Template"
output:
  bookdown::gitbook: default
documentclass: book
date: "`r Sys.Date()`"
---

```{r, include=FALSE}

knitr::opts_chunk$set(message=FALSE, warning=FALSE, echo = FALSE)
options(scipen=999)  # turn-off scientific notation like 1e+48

# Java must be installed as a prerequisite
library(rJava)

library(tidyverse)
library(tidytext)
library(dplyr)
library(stringr)
library(lubridate)
library(gutenbergr)
library(janeaustenr)
library(NLP)
library(openNLP)
library(NLP)
library(tm)
library(topicmodels)
library(quanteda)
library(text2vec)
library(spacyr)
library(RWeka)
# library(RcmdrPlugin.temis)
library(tm)
library(languageR)
library(koRpus)
library(RKEA)
#library(maxent)
library(lsa)
library(wordcloud)
library(syuzhet)
library(SnowballC)
library(textfeatures)
library(SentimentAnalysis)
library(crfsuite)

```




```{r}

## SET PARAMETERS

focus_question <- 11
question_language <- "en"

question_list <- read.csv("list-of-questions.csv", header = TRUE, stringsAsFactors = FALSE, encoding="UTF-8")
#question_list

question_filter <- filter(question_list, QID == focus_question)
question_filter$Question

q_keyword <- question_filter$Keyword


```




```{r}

## INGEST DATA

data_file <- paste0("./outputs/q-", focus_question, "-lang.csv")

corpus_csv <- read.csv(data_file, header = TRUE, stringsAsFactors = FALSE, encoding="UTF-8")

# Filter english
corpus_csv <- filter(corpus_csv, cld2 == question_language)
#corpus_csv

```




```{r, echo=FALSE, message=FALSE, fig.width=7, fig.height=10}

# http://htmlpreview.github.io/?https://github.com/kasperwelbers/corpustools/blob/master/vignettes/corpustools.html
tc = create_tcorpus(corpus_csv, doc_column = 'row', text_columns = 'Comment')
# tc
# head(tc$tokens)
# head(tc$meta)

tc$preprocess(use_stemming = T, remove_stopwords=T)
tc$preprocess(column = 'token', new_column = 'feature', remove_stopwords=T, use_stemming=T, min_docfreq = 5)

dfm = get_dfm(tc, 'feature')
dfm

##  Use sentences as rows instead of documents
# dfm_sent = get_dfm(tc, 'feature', context_level = 'sentence')

##  Use a weighting scheme. 
dfm_weighted = get_dfm(tc, 'feature', weight = 'tfidf')

## fit lda model, using the create_feature argument to store the topic assignments
m <- tc$lda_fit('feature', create_feature = 'topic', K = 5, alpha = 0.001)
head(tc$tokens, 10)

queries = data.frame(label = c('Tolerate', 'Negations', 'Observed'),
                     query = c('tolerate*', 'not OR doesn\'t', 'see OR happen*'))
# hits <- search_features(tc, query=queries$query, code=queries$label)
hits <- search_features(tc, query=queries$query)
hits

# count_tcorpus(tc, hits=hits)
g <- semnet(hits, measure = 'con_prob')
g

igraph::get.adjacency(g, attr = 'weight')
plot(hits)


```



```{r, echo=FALSE, message=FALSE, fig.width=7, fig.height=10}

# Semantic networks based on co-occurrence
gf = semnet_window(tc, 'feature')
gb = backbone_filter(gf, alpha = 0.001, max_vertices = 100)
plot_semnet(gb)


# "Strongly Agree", "Agree", "Neither Agree Nor Disagree", "Disagree", "Strongly Disagree", "Not Recorded"
strongly_agree_comp = compare_subset(tc, feature='feature', subset_meta_x = Answer == "Strongly Agree")
plot(strongly_agree_comp)

agree_comp = compare_subset(tc, feature='feature', subset_meta_x = Answer == "Agree")
plot(agree_comp)

neutral_comp = compare_subset(tc, feature='feature', subset_meta_x = Answer == "Neither Agree Nor Disagree")
plot(neutral_comp)

disagree_comp = compare_subset(tc, feature='feature', subset_meta_x = Answer == "Disagree")
plot(disagree_comp)

strongly_disagree_comp = compare_subset(tc, feature='feature', subset_meta_x = Answer == "Strongly Disagree")
plot(strongly_disagree_comp)

```



