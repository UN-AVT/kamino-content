---
title: "Scraping Sudans Post"
author: "Min Thiha Myo"

output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(message=FALSE, warning=FALSE, echo=TRUE, class.source= 'fold-hide')
options(scipen=999)  # turn-off scientific notation like 1e+48

# Clear environment and memory
rm(list = ls(all.names = TRUE)) #will clear all objects includes hidden objects.
gc() #free up memory and report the memory usage.

library(tidyverse)


```


## Scrape sudanspost.com for the headlines, links, date, summary and body

***

```{r , echo=FALSE, message=FALSE, warning=FALSE,class.source = 'fold-hide'}

print(paste0("Date:", as.character(Sys.Date())))

library(rvest)
library(dplyr)

url = "https://www.sudanspost.com/"
page = read_html(url)

headlines <- page %>%
  html_nodes(".jeg_post_title a") %>%
  html_text()

links <- page %>%
  html_nodes(".jeg_post_title a") %>%
  html_attr("href")



sudan_post <- data.frame(headlines,links)


summary <- list()
timearticle <- list()
body <- list()

for (i in sudan_post$links){
  url1 <- read_html(i)
 
   summ <- url1 %>%
    html_node(".jeg_post_subtitle") %>%
    html_text()
  summary <- append(summary, summ)
  
  datar <- url1 %>%
    html_node(".jeg_meta_date a") %>%
    html_text()
  timearticle <- append(timearticle, datar)
  
  bodies <- url1 %>%
    html_node(".content-inner p")%>%
    html_text()
  bodies <- paste (bodies, collapse = " ")
  body <- append(body, bodies)
  
}

sudan_post$summary <- as.character(summary)
sudan_post$body <- as.character(body)
sudan_post$date <- as.character(timearticle)




csvname <- as.character(Sys.Date())
csvname2 <- paste0(csvname, "sudanpost.csv")
write.csv(sudan_post, csvname2)
```


# Headings of Sudans Post

```{r , echo=TRUE, message=FALSE, warning=FALSE,class.source = 'fold-hide'}

head(sudan_post$headlines)


```

# News Articles of Sudans Post

```{r , echo=TRUE, message=FALSE, warning=FALSE,class.source = 'fold-hide'}

head(sudan_post$body)

```

# Keyword Catcher

## We will catch any article that contains the word "covid"

```{r , echo=TRUE, message=FALSE, warning=FALSE,class.source = 'fold-hide'}

test <- sudan_post
test$body <- tolower(test$body)

x <- grep("covid", test$body)
email <- sudan_post[x,]
email


```

## We will start implementing text analytics on the Headlines

## We use library(tm) to remove stopwords, punctuation and custom words

## Then we will draw a word frequncy plot

```{r , echo=FALSE, message=FALSE, warning=FALSE,class.source = 'fold-hide', fig.width=13, fig.height=11}

library(tm)

TextDoc <- VCorpus(VectorSource(sudan_post$headlines))

# Convert the text to lower case
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
# Remove numbers
TextDoc <- tm_map(TextDoc, removeNumbers)
# Remove english common stopwords
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
# Remove punctuations
TextDoc <- tm_map(TextDoc, removePunctuation)
# Remove your own stop word ("South" and "sudan")
TextDoc <- tm_map(TextDoc, removeWords, c("south", "sudan", "sudans", "sudanese", "breaking", "us", "united states", "says")) 

# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)


# Plot the most frequent words

jpegname <- paste0(csvname, "headlines_word_frequency.jpeg")
jpeg(jpegname, width = 800, height = 600, quality = 100)
plot1 <- barplot(dtm_d[1:15,]$freq, las = 2, names.arg = dtm_d[1:15,]$word,
        col ="steelblue", main ="Top 15 most frequent words",
        ylab = "Word frequencies")
dev.off()

plot1 <- barplot(dtm_d[1:15,]$freq, las = 2, names.arg = dtm_d[1:15,]$word,
        col ="steelblue", main ="Top 15 most frequent words",
        ylab = "Word frequencies")


```

# Another way of plotting the Word Frequency is Word Clouds

```{r , echo=FALSE, message=FALSE, warning=FALSE,class.source = 'fold-hide', fig.width=13, fig.height=11}

library(wordcloud)

set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 3,
          random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))


```


# We will now do some sentiment analysis


```{r , echo=FALSE, message=FALSE, warning=FALSE,class.source = 'fold-hide', fig.width=13, fig.height=11}
library("syuzhet")

ss <- sudan_post$headlines

syuzhet_vector <- get_sentiment(ss, method="syuzhet")

# see summary statistics of the vector
sahead <- summary(syuzhet_vector)
sahead

d<-get_nrc_sentiment(ss)
library(ggplot2)
td<-data.frame(t(d))
#The function rowSums computes column sums across rows for each level of a grouping variable.
td_new <- data.frame(rowSums(td))
#Transformation and cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new3<-td_new[1:8,]
#Plot One - count of words associated with each sentiment

jpegname2 <- paste0(csvname, "sentiment_headlines.jpeg")
jpeg(jpegname2, width = 800, height = 600, quality = 100)

quickplot(sentiment, data=td_new3, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Emotion sentiments of Headlines")
dev.off()

quickplot(sentiment, data=td_new3, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Emotion sentiments of Headlines")

```


# Let's move on to the body

```{r , echo=FALSE, message=FALSE, warning=FALSE,class.source = 'fold-hide', fig.width=13, fig.height=11}

library(tm)

TextDoc2 <- VCorpus(VectorSource(sudan_post$body))

# Convert the text to lower case
TextDoc2 <- tm_map(TextDoc2, content_transformer(tolower))
# Remove numbers
TextDoc2 <- tm_map(TextDoc2, removeNumbers)
# Remove english common stopwords
TextDoc2 <- tm_map(TextDoc2, removeWords, stopwords("english"))
# Remove punctuations
TextDoc2 <- tm_map(TextDoc2, removePunctuation)
# Remove your own stop word ("South" and "sudan")
TextDoc2 <- tm_map(TextDoc2, removeWords, c("south", "sudan", "sudans", "sudanese", "breaking", "us", "united states", "says", "photo", "president")) 


# Build a term-document matrix
TextDoc_dtm2 <- TermDocumentMatrix(TextDoc2)
dtm_m2 <- as.matrix(TextDoc_dtm2)
# Sort by descearing value of frequency
dtm_v2 <- sort(rowSums(dtm_m2),decreasing=TRUE)
dtm_d2 <- data.frame(word = names(dtm_v2),freq=dtm_v2)
# Display the top 5 most frequent words
head(dtm_d2, 5)


# Plot the most frequent words

jpegname3 <- paste0(csvname, "body_word_frequency.jpeg")
jpeg(jpegname3, width = 800, height = 600, quality = 100)

barplot(dtm_d2[1:15,]$freq, las = 2, names.arg = dtm_d2[1:15,]$word,
        col ="lightgreen", main ="Top 10 most frequent words of Body",
        ylab = "Word frequencies")
dev.off()

barplot(dtm_d2[1:15,]$freq, las = 2, names.arg = dtm_d2[1:15,]$word,
        col ="lightgreen", main ="Top 10 most frequent words of Body",
        ylab = "Word frequencies")

```



```{r , echo=FALSE, message=FALSE, warning=FALSE,class.source = 'fold-hide', fig.width=13, fig.height=11}
set.seed(1234)
wordcloud(words = dtm_d2$word, freq = dtm_d2$freq, min.freq = 8,
          random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))

```


# Bi-Gram Analysis


```{r , echo=FALSE, message=FALSE, warning=FALSE,class.source = 'fold-hide', fig.width=13, fig.height=11}
library(tidytext)
doc_co <- Corpus(VectorSource(sudan_post$body))

doc_co <- tm_map(doc_co, removePunctuation)
doc_co <- tm_map(doc_co, content_transformer(tolower))
doc_co  <- tm_map(doc_co , removeWords, stopwords("english"))
doc_co  <- tm_map(doc_co , removePunctuation)

doc_co  <- tm_map(doc_co , removeWords, c("south", "sudan", "sudans", "sudanese", "breaking", "us", "united states", "says", "photo")) 
text_df <- data.frame(text_clean = get("content", doc_co), stringsAsFactors = FALSE)


bigram <- text_df %>% unnest_tokens(bigram, text_clean, token = "ngrams", n = 2)

bigram_count <- bigram  %>% count(bigram, sort = TRUE)


par(mar=c(11,4,4,4))
barplot(bigram_count[1:15,]$n, las=2, names.arg = bigram_count[1:15,]$bigram,
        col ="steelblue", main ="Bi-Gram Word Analysis",
        ylab = "Bi-Gram Word frequencies")
dev.off()


```




# Sentiment Analysis

```{r , echo=FALSE, message=FALSE, warning=FALSE,class.source = 'fold-hide', fig.width=13, fig.height=11}
library("syuzhet")

ss2 <- sudan_post$body

syuzhet_vector2 <- get_sentiment(ss2, method="syuzhet")

# see summary statistics of the vector
sabody <- summary(syuzhet_vector2)
sabody

d2<-get_nrc_sentiment(ss2)
library(ggplot2)
td2<-data.frame(t(d2))
#The function rowSums computes column sums across rows for each level of a grouping variable.
td_new2 <- data.frame(rowSums(td2))
#Transformation and cleaning
names(td_new2)[1] <- "count"
td_new2 <- cbind("sentiment" = rownames(td_new2), td_new2)
rownames(td_new2) <- NULL
td_new4<-td_new2[1:8,]
#Plot One - count of words associated with each sentiment

jpegname4 <- paste0(csvname, "sentiment_body.jpeg")
jpeg(jpegname4, width = 800, height = 600, quality = 100)

quickplot(sentiment, data=td_new4, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Emotion sentiments of Body")
dev.off()

quickplot(sentiment, data=td_new4, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Emotion sentiments of Body")
##########################################################


textname <- paste0(csvname, "sentiment.txt")
sink(textname)
cat("Sentiment Analysis of Headlines")
cat("\n")
cat("Min. 1st Qu.  Median    Mean 3rd Qu.    Max. ")
cat("\n")
cat(sahead)
cat("\n")
cat("Sentiment Analysis of Body")
cat("\n")
cat("Min. 1st Qu.  Median    Mean 3rd Qu.    Max. ")
cat("\n")
cat(sabody)
sink()


```
