---
pagetitle: "Kamino"
output: 
  html_document:
    theme: lumen
    #css: ["../../z-assemblers/assets/styles/content/kamino.css", #"../../z-assemblers/assets/icons/line-awesome/css/line-awesome.css"]
    df_print: paged
    mathjax: NULL
    code_folding: show
    include:
      #in_header: "../../z-assemblers/fragments/header.html"
      #after_body: "../../z-assemblers/fragments/footer.html"
    self_contained: true
    lib_dir: libs
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, echo=FALSE, class.source = 'fold-hide',
   dev = "svg")
options(scipen=999)  # turn-off scientific notation like 1e+48

# Clear environment and memory
rm(list = ls(all.names = TRUE)) #will clear all objects includes hidden objects.
gc() #free up memory and report the memory usage.

# load twitter library - the rtweet library is recommended now over twitteR
library(rtweet)
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
# text mining library
library(tidytext)
# plotting packages
library(igraph)
library(ggraph)
library(ggiraph)
library(wordcloud)
library(tidyverse)
library(textstem)
library(quanteda)
library(openNLP)
library(NLP)
library(rJava)
```


<div class="activity">
TEXT ANALYSIS  
</div>

***

## Ingest tweets

```{r}
# Search for up to 10,000 tweets containing #covid19, excluding re-tweets:
# A browser window will pop up to request for authorization
#
# covid_tweets <- search_tweets(q = "#covid19", n = 10000,
#                                     lang = "en",
#                                     include_rts = FALSE)
# head(covid_tweets)

# You can use the same method to get your own most recent dataset, we saved the search results from Aug 30, and only selected the first 10 columns as our dataset and ingest the dataset as below shows.
```


```{r}

covid_tweets<-read_csv("archetypes/covid_tweets.csv")
head(covid_tweets)
```

## Clean up
Remove URLs
```{r}

covid_tweets_text<-covid_tweets%>%select(text)
covid_tweets_text$stripped_text <- gsub("http.*","",  covid_tweets_text$text)
covid_tweets_text$stripped_text <- gsub("https.*","", covid_tweets_text$stripped_text)

head(covid_tweets_text)
```
## Tokenize
```{r}
# remove punctuation, convert to lowercase
covid_tweets_text_clean <- covid_tweets_text %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
covid_tweets_text_clean
```



```{r}

# load list of stop words - from the tidytext package
data("stop_words")
# view first 6 words
head(stop_words)




# remove stop words from your list of words
cleaned_tweet_words <- covid_tweets_text_clean %>%
  anti_join(stop_words)

# customize words to remove
c_list<-c('covid19','covid','19','amp')
cleaned_tweet_words <- cleaned_tweet_words %>% filter(!word %in% c_list)
cleaned_tweet_words
```



# Lemmatize and Stemming

```{r}

lemmatize <- lemmatize_words(cleaned_tweet_words$word)
as.data.frame(lemmatize)

porter <- stem_strings(cleaned_tweet_words$word, language = "porter")
as.data.frame(porter)


```


# Term Frequency

```{r}

word_count <- cleaned_tweet_words %>% count(word, sort = TRUE) 
word_count

word_count_lemmatize <- as.data.frame(lemmatize) %>% count(lemmatize, sort = TRUE) 
word_count_lemmatize

total_words <- word_count %>% summarize(total = sum(n))

# Tally Measure
tally <- word_count %>% mutate(total = total_words$total)
tally

# By Rank Measure
freq_by_rank <- tally %>% mutate(rank = row_number(), `term frequency` = n/total)
freq_by_rank
```

# Generate a word cloud

```{r fig.height=6, fig.width=12}


set.seed(1234)
wordcloud(words = word_count_lemmatize$lemmatize, freq = word_count_lemmatize$n, min.freq = 20,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))

```

# Plot the most frequent 50 words

```{r fig.height=6, fig.width=12}

v1 <- ggplot(word_count_lemmatize[1:50,], aes(x=reorder(lemmatize, -n), y=n)) +
  geom_bar(stat="identity", width=0.6, fill="darkseagreen3")  +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=90, hjust=1, vjust=0.2),
        plot.caption = element_text(hjust = 0),
        panel.grid.major.x = element_blank()) +
  xlab("Lemmatized Word") + # for the x axis label
  ylab("Frequency") + # for the y axis label
  coord_cartesian(ylim=c(0, 1500))

# Print the plot
girafe(ggobj = v1, width_svg = 13, height_svg = 7,
       options = list(opts_sizing(rescale = TRUE, width = 1.0)))
```




# Bigrams

```{r}

bigram <- covid_tweets_text %>% unnest_tokens(bigram, stripped_text, token = "ngrams", n = 2)


# Remove NA values
bigram <- bigram %>% drop_na()


bigram_count <- bigram %>% count(bigram, sort = TRUE)
bigram_count





```


### Remove stop words

```{r}


 bigrams_separated <- bigram %>% separate(bigram, c("word1", "word2"), sep = " ")
 bigrams_filtered <- bigrams_separated %>% filter(!word1 %in% stop_words$word) %>% filter(!word2 %in% stop_words$word)
 
 # new bigram counts:
 bigram_counts <- bigrams_filtered %>% count(word1, word2, sort = TRUE)
 
 bigrams_united <- bigrams_filtered %>% unite(bigram, word1, word2, sep = " ")%>% count(bigram, sort = TRUE)
 bigrams_united



```

```{r  fig.height=12, fig.width=12}

v2 <- ggplot(bigrams_united[1:50,], aes(x=reorder(bigram, n), y=n)) +
  geom_bar(stat="identity", width=0.6, fill="darkseagreen2")  +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust=1, vjust=0.2),
        plot.caption = element_text(hjust = 0),
        panel.grid.major.x = element_blank()) +
  xlab("Bigram") + # for the x axis label
  ylab("Frequency") + # for the y axis label
  
  coord_flip()

# Print the plot
girafe(ggobj = v2, width_svg = 13, height_svg = 13,
       options = list(opts_sizing(rescale = TRUE, width = 1.0)))
```






# Sentiment Analysis

```{r}

## LOAD MODELS

# From References
afinn_165 <- read.csv("refs/sentiment/afinn/afinn-165-en.csv", header = TRUE, stringsAsFactors = FALSE, encoding="UTF-8")
#names(afinn_165)

# opinion_words <- read.csv("refs/sentiment/opinion-lexicon/opinion-words.csv", header = TRUE, stringsAsFactors = FALSE, encoding="UTF-8")
# #names(opinion_words)
# 
# pattern <- read.csv("refs/sentiment/pattern/pattern_en.csv", header = TRUE, stringsAsFactors = FALSE, encoding="UTF-8")
# #names(pattern)
# 
# senticon <- read.csv("refs/sentiment/senticon/senticon.csv", header = TRUE, stringsAsFactors = FALSE, encoding="UTF-8")
# #names(senticon)

```

```{r}

word_count<- word_count_lemmatize %>% rename(word='lemmatize')

afinn_165_sentiment <- word_count %>% inner_join(afinn_165) 
afinn_165_sentiment

# opinion_words_sentiment <- word_count %>% inner_join(opinion_words) 
# #opinion_words_sentiment
# 
# pattern_sentiment <- word_count %>% inner_join(pattern) 
# #pattern_sentiment
# 
# senticon_sentiment <- word_count %>% inner_join(senticon) 
# #senticon_sentiment

```

# Make the plot

```{r  fig.height=6, fig.width=12}

df<-afinn_165_sentiment
df$sentiment <- factor(df$sentiment)
df <- df %>% mutate(id = row_number())

afinn_165_v <- ggplot(data=df, aes(x=sentiment, y=n, fill=sentiment)) +
    geom_boxplot(alpha=0.5) +
    scale_y_continuous(trans='log2') +
    geom_jitter_interactive(aes(color=sentiment, tooltip = word, data_id = id), size=1, alpha=0.8) +
    theme_minimal() +
    theme(panel.background = element_blank()) +
  theme(panel.grid.major.x = element_blank()) +
  theme(panel.grid.minor.x = element_blank()) +
  theme(panel.grid.major.y = element_blank()) +
  theme(panel.grid.minor.y = element_blank()) +
    theme(text = element_text(size=20),
      legend.position="none",
      plot.title = element_text(size=11)
    ) +
    xlab("")

girafe(ggobj = afinn_165_v, width_svg = 13, height_svg = 7,
       options = list(opts_sizing(rescale = TRUE, width = 1.0)))

```

# Positive sentiment words

```{r}

pos <- afinn_165_sentiment %>% filter(sentiment > 0)
pos


```

# Negative sentiment words

```{r}

neg <- afinn_165_sentiment %>% filter(sentiment < 0)
neg

```

# Bi-Gram Negation Words

### NOT

```{r}

not <- bigrams_separated %>% filter(word1 == "not") %>% count(word1, word2, sort = TRUE)


not_words <- not %>% inner_join(afinn_165, by = c('word2' = 'word')) %>% mutate (sentiment_updated = sentiment * (-1)) %>% unite(words, word1, word2, sep = " ")
not_words


```

```{r  fig.height=12, fig.width=12}

v3 <- ggplot(not_words[1:10,], aes(x=reorder(words, n), y=n)) +
  geom_bar(stat="identity", width=0.6, fill="darkseagreen2")  +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust=1, vjust=0.2),
        plot.caption = element_text(hjust = 0),
        panel.grid.major.x = element_blank()) +
  xlab("Not words") + # for the x axis label
  ylab("Frequency") + # for the y axis label
  
  coord_flip()

# Print the plot
girafe(ggobj = v3, width_svg = 13, height_svg = 7,
       options = list(opts_sizing(rescale = TRUE, width = 1.0)))
```


### NO

```{r}

no <- bigrams_separated %>% filter(word1 == "no") %>% count(word1, word2, sort = TRUE)


no_words <- no %>% inner_join(afinn_165, by = c('word2' = 'word')) %>% mutate (sentiment_updated = sentiment * (-1)) %>% unite(words, word1, word2, sep = " ")
  
no_words

```

```{r fig.height=12, fig.width=12}

v4 <- ggplot(no_words[1:10,], aes(x=reorder(words, n), y=n)) +
  geom_bar(stat="identity", width=0.6, fill="darkseagreen2")  +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust=1, vjust=0.2),
        plot.caption = element_text(hjust = 0),
        panel.grid.major.x = element_blank()) +
  xlab("No words") + # for the x axis label
  ylab("Frequency") + # for the y axis label
  
  coord_flip()

# Print the plot
girafe(ggobj = v4, width_svg = 13, height_svg = 7,
       options = list(opts_sizing(rescale = TRUE, width = 1.0)))
```

### NEVER

```{r}

never <- bigrams_separated %>% filter(word1 == "never") %>% count(word1, word2, sort = TRUE)


 never_words <- never %>% inner_join(afinn_165, by = c('word2' = 'word')) %>% mutate (sentiment_updated = sentiment * (-1))%>% unite(words, word1, word2, sep = " ")

 never_words

```

```{r fig.height=12, fig.width=12}

v4 <- ggplot(never_words, aes(x=reorder(words, n), y=n)) +
  geom_bar(stat="identity", width=0.6, fill="darkseagreen2")  +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust=1, vjust=0.2),
        plot.caption = element_text(hjust = 0),
        panel.grid.major.x = element_blank()) +
  xlab("Never words") + # for the x axis label
  ylab("Frequency") + # for the y axis label
  
  coord_flip()

# Print the plot
girafe(ggobj = v4, width_svg = 13, height_svg = 7,
       options = list(opts_sizing(rescale = TRUE, width = 1.0)))
```

### WITHOUT

```{r}

without <- bigrams_separated %>% filter(word1 == "without") %>% count(word1, word2, sort = TRUE)
#without

without_words <- without %>% inner_join(afinn_165, by = c('word2' = 'word')) %>% mutate (sentiment_updated = sentiment * (-1))%>% unite(words, word1, word2, sep = " ")

without_words

```

```{r  fig.height=7, fig.width=12}

v5 <- ggplot(without_words, aes(x=reorder(words, n), y=n)) +
  geom_bar(stat="identity", width=0.6, fill="darkseagreen2")  +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust=1, vjust=0.2),
        plot.caption = element_text(hjust = 0),
        panel.grid.major.x = element_blank()) +
  xlab("Without words") + # for the x axis label
  ylab("Frequency") + # for the y axis label
  
  coord_flip()

# Print the plot
girafe(ggobj = v5, width_svg = 13, height_svg = 7,
       options = list(opts_sizing(rescale = TRUE, width = 1.0)))
```

# NER
```{r}
############################################### Before you start   ###################################################

# Run this code in your console first "install.packages("openNLPmodels.en", dependencies=TRUE, repos =   "http://datacube.wu.ac.at/")"
# Navigate to your r library folder, and create a new folder within "openNLPmodels.en" called "models"
# Go to http://opennlp.sourceforge.net/models-1.5/, and download pre-trained models as you wish, and save them in the folder you just created.

######################################################################################################################


#only select 50 tweets as an example here, since it consumes too much computing power for the entire dataset.

text <- paste(covid_tweets_text$stripped_text[1:50], collapse = " ")
#converts variable into a string
text <- as.String(text)

#create annotators for words and sentences
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
pos_tag_ann <- Maxent_POS_Tag_Annotator(language = "en", probs = FALSE, model = NULL)


#creates annotators of kind person, location and organization
organization_ann <- Maxent_Entity_Annotator(kind = "organization")
person_ann <- Maxent_Entity_Annotator(kind = "person")
location_ann <- Maxent_Entity_Annotator(kind = "location")
date_ann <- Maxent_Entity_Annotator(kind = "date")
percentage_ann <- Maxent_Entity_Annotator(kind = "percentage")
money_ann <- Maxent_Entity_Annotator(kind = "money")

#holds annotators in the order to be applied
pipeline <- list(sent_ann,
                 word_ann,
                 pos_tag_ann,
                 person_ann,
                 location_ann,
                organization_ann,
                 date_ann,
                 percentage_ann,
                money_ann
                )

annotated_text <- annotate(text, pipeline)

```


```{r}

k <- sapply(annotated_text$features, `[[`, "kind")
organizations <- text[annotated_text[k == "organization"]]
people <- text[annotated_text[k == "person"]]
locations <- text[annotated_text[k == "location"]]
date <- text[annotated_text[k == "date"]]
percentage <- text[annotated_text[k == "percentage"]]
money <- text[annotated_text[k == "money"]]



if(length(organizations)!=0){
organizations <- as.data.frame(as.character(organizations))
names(organizations)[1] <- "word"
organizations$ner <- "ORGANIZATION"
organizations
}

if(length(people)!=0){
people <- as.data.frame(as.character(people))
names(people)[1] <- "word"
people$ner <- "PEOPLE"
people
}

if(length(locations)!=0){
locations <- as.data.frame(as.character(locations))
names(locations)[1] <- "word"
locations$ner <- "LOCATION"
locations
}

if(length(date)!=0){
date<- as.data.frame(as.character(date))
names(date)[1] <- "word"
date$ner <- "DATE"
date
}

if (length(percentage)!=0) {
percentage <- as.data.frame(as.character(percentage))
names(percentage)[1] <- "word"
percentage$ner <- "PERCENTAGE"
percentage
}

if (length(money)!=0) {
money <- as.data.frame(as.character(money))
names(money)[1] <- "word"
money$ner <- "MONEY"
money
}

ner_df <- rbind(organizations, people, locations, date, percentage, money)

```

# Parts of Speech

```{r}
pos_tags <- sapply(annotated_text$features, `[[`, "POS")
noun_filter = c("NN", "NNP", "NNS", "NNPS")
adjective_filter = c("JJ", "JJR", "JJS")
verb_filter = c("VB", "VBD", "VBG", "VBN", "VBP", "VBZ")
adverb_filter = c("RB", "RBR", "RBS")
pronoun_filter = c("PRP", "PRP$", "WP", "WP$")

noun_pos_res <- text[annotated_text[pos_tags %in% noun_filter]]
noun_pos_res <- as.data.frame(noun_pos_res)
names(noun_pos_res)[1] <- "word"
noun_pos_res$pos_tag <- "NOUN"
#noun_pos_res

adj_pos_res <- text[annotated_text[pos_tags %in% adjective_filter]]
adj_pos_res <- as.data.frame(adj_pos_res)
names(adj_pos_res)[1] <- "word"
adj_pos_res$pos_tag <- "ADJECTIVE"
#adj_pos_res

verb_pos_res <- text[annotated_text[pos_tags %in% verb_filter]]
verb_pos_res <- as.data.frame(verb_pos_res)
names(verb_pos_res)[1] <- "word"
verb_pos_res$pos_tag <- "VERB"
#verb_pos_res

adverb_pos_res <- text[annotated_text[pos_tags %in% adverb_filter]]
adverb_pos_res <- as.data.frame(adverb_pos_res)
names(adverb_pos_res)[1] <- "word"
adverb_pos_res$pos_tag <- "ADVERB"
#adverb_pos_res

pronoun_pos_res <- text[annotated_text[pos_tags %in% pronoun_filter]]
pronoun_pos_res <- as.data.frame(pronoun_pos_res)
names(pronoun_pos_res)[1] <- "word"
pronoun_pos_res$pos_tag <- "PRONOUN"
#pronoun_pos_res

pos_df <- rbind(noun_pos_res, adj_pos_res, verb_pos_res, adverb_pos_res, pronoun_pos_res)
pos_df
```


# Keyword In Context (KWIC)

```{r, echo=FALSE, message=FALSE, rows.print=20, class.source='fold-hide'}


keyword <- c('kids*')

kwic_keyword <- kwic(covid_tweets_text$stripped_text, pattern = keyword, valuetype = "regex")
#names(kwic_keyword)

kwic_keyword <- data.frame(kwic_keyword, stringsAsFactors=F)
# kwic_keyword

vars <- c("pre", "keyword", "post")
kwic_select <- kwic_keyword  %>% select(one_of(vars))
# kwic_select

kwic_select$kwic <- paste0( kwic_select$pre, " | ", kwic_select$keyword, " | ", kwic_select$post )
kwic_print <- kwic_select  %>% select(kwic)


DT::datatable(kwic_print)


```


# References
## The citations and data sources used for this case
[Collecting Twitter Data](https://cran.r-project.org/web/packages/rtweet/vignettes/intro.html)

```{r generateBibliography, echo=FALSE, message=FALSE, warning=FALSE}

# cleanbib()
# options("citation_format" = "pandoc")


```


```{js, message=FALSE, warning=FALSE, echo=FALSE}

// Must be included to position footer
$(function() {
  $('.main-container').after($('.footer'));
})

```

