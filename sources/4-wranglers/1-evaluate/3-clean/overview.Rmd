---
title: "Kamino"
output:
  bookdown::gitbook: default
documentclass: book
date: "`r Sys.Date()`"
---

```{r, include=FALSE}

knitr::opts_chunk$set(message=FALSE, warning=FALSE, echo=FALSE)
options(scipen=999)  # turn-off scientific notation like 1e+48

library(showtext)
## Loading Google fonts (https://fonts.google.com/)
font_add_google("Inconsolata", "inconsolata")
# turn on showtext
showtext_auto()

```

WRANGLERS  

# Clean data
## Remove duplicates, remove missing values, replace missing values

```{r, out.width="20%", fig.align="center"}

masthead <- "assets/noun_Clean_2919611.svg"
knitr::include_graphics(masthead, dpi=72)

```

After __profiling__ a data source data __quality__ issues may be identified requiring the use of cleaning methods. Typical issues include __duplicate__ records, __missing__ values, and __inconsistent__ or __invalid__ values. Data cleansing provides higher quality and consistent data for downstream analysis.

Taking a real-life case as an example, let's take a look at this data file from 
These are the key areas we'll focus on addressing:

* __De-Duplicate__ : eliminate duplicate records by removing or establishing a unique identifier
* __Missing Values__ : detect null, blank, or empty cells and apply a logic to remove or replace them with new defined or _imputed_ values
* __Invalid Values__ : detect data points that may have been mis-keyed during data entry

Each of these areas have a solution for either __removing__ or __replacing__ the subject values. When a replacement is the preferred
method, imputing a value involves making a reasonable guess at the missing value based on a method of analytic reinforcement. These are the
common approaches to consider:

* using a measure of __central tendency__ (mean, median, mode)
* identify the most __similar cases__ to suggest a candidate value for substitution
* assign a value based on the __probability distribution__ of the non-missing data
* derive the value based on __other features__
* use a __classifier__ model
